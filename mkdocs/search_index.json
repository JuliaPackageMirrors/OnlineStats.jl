{
    "docs": [
        {
            "location": "/", 
            "text": "OnlineStats.jl\n\n\nOnlineStats\n is a Julia package which provides online algorithms for statistical models.\n\n\nOnline algorithms are well suited for streaming data or when data is too large to hold in memory.\n\n\nObservations are processed one at a time and all \nalgorithms use O(1) memory\n.\n\n\nOverview\n\n\nEvery OnlineStat is a Type\n\n\nusing OnlineStats\no = Mean()\n\n\n\n\nAll OnlineStats can be updated\n\n\ny = randn(100)\ny2 = randn(100)\n\n# update Mean\nfor yi in y\n    fit!(o, yi)\nend\nfor yi in y2\n    fit!(o, yi)\nend\n\n# or more simply:\nfit!(o, y)\nfit!(o, y2)\n\n\n\n\nOnlineStats share a common interface\n\n\n\n\nvalue(o)\n\n\nthe associated value of an OnlineStat\n\n\n\n\n\n\nnobs(o)\n\n\nthe number of observations seen\n\n\n\n\n\n\n\n\nWhat Can OnlineStats Do?\n\n\nWhile many estimates can be calculated analytically with an online algorithm, several\ntype rely on stochastic approximation.\n\n\nSummary Statistics\n\n\n\n\nMean: \nMean\n, \nMeans\n\n\nVariance: \nVariance\n, \nVariances\n\n\nQuantiles: \nQuantileMM\n, \nQuantileSGD\n\n\nCovariance Matrix: \nCovMatrix\n\n\nMaximum and Minimum:  \nExtrema\n\n\nSkewness and Kurtosis:  \nMoments\n\n\nSum/Differences:  \nSum\n, \nSums\n, \nDiff\n, \nDiffs\n\n\n\n\nDensity Estimation\n\n\n\n\ndistributionfit(D, data)\n\n\nFor \nD in [Beta, Categorical, Cauchy, Gamma, LogNormal, Normal, Multinomial, MvNormal]\n\n\n\n\n\n\nGaussian Mixtures: \nNormalMix\n\n\n\n\nPredictive Modeling\n\n\n\n\nLinear Regression: \nLinReg\n, \nStatLearn\n\n\nLogistic Regression: \nStatLearn\n\n\nPoisson Regression: \nStatLearn\n\n\nSupport Vector Machines: \nStatLearn\n\n\nQuantile Regression: \nStatLearn\n, \nQuantReg\n\n\nHuber Loss Regression: \nStatLearn\n\n\nL1 Loss Regression: \nStatLearn\n\n\n\n\nExperimental Features\n\n\n\n\nK-Means clustering: \nKMeans\n\n\nBootstrapping: \nBernoulliBootstrap\n, \nPoissonBootstrap\n\n\nApproximate count of distinct elements: \nHyperLogLog\n\n\nVisualizing value of OnlineStats: \nTracePlot\n, \nCompareTracePlot", 
            "title": "Home"
        }, 
        {
            "location": "/#onlinestatsjl", 
            "text": "OnlineStats  is a Julia package which provides online algorithms for statistical models.  Online algorithms are well suited for streaming data or when data is too large to hold in memory.  Observations are processed one at a time and all  algorithms use O(1) memory .", 
            "title": "OnlineStats.jl"
        }, 
        {
            "location": "/#overview", 
            "text": "", 
            "title": "Overview"
        }, 
        {
            "location": "/#every-onlinestat-is-a-type", 
            "text": "using OnlineStats\no = Mean()", 
            "title": "Every OnlineStat is a Type"
        }, 
        {
            "location": "/#all-onlinestats-can-be-updated", 
            "text": "y = randn(100)\ny2 = randn(100)\n\n# update Mean\nfor yi in y\n    fit!(o, yi)\nend\nfor yi in y2\n    fit!(o, yi)\nend\n\n# or more simply:\nfit!(o, y)\nfit!(o, y2)", 
            "title": "All OnlineStats can be updated"
        }, 
        {
            "location": "/#onlinestats-share-a-common-interface", 
            "text": "value(o)  the associated value of an OnlineStat    nobs(o)  the number of observations seen", 
            "title": "OnlineStats share a common interface"
        }, 
        {
            "location": "/#what-can-onlinestats-do", 
            "text": "While many estimates can be calculated analytically with an online algorithm, several\ntype rely on stochastic approximation.", 
            "title": "What Can OnlineStats Do?"
        }, 
        {
            "location": "/#summary-statistics", 
            "text": "Mean:  Mean ,  Means  Variance:  Variance ,  Variances  Quantiles:  QuantileMM ,  QuantileSGD  Covariance Matrix:  CovMatrix  Maximum and Minimum:   Extrema  Skewness and Kurtosis:   Moments  Sum/Differences:   Sum ,  Sums ,  Diff ,  Diffs", 
            "title": "Summary Statistics"
        }, 
        {
            "location": "/#density-estimation", 
            "text": "distributionfit(D, data)  For  D in [Beta, Categorical, Cauchy, Gamma, LogNormal, Normal, Multinomial, MvNormal]    Gaussian Mixtures:  NormalMix", 
            "title": "Density Estimation"
        }, 
        {
            "location": "/#predictive-modeling", 
            "text": "Linear Regression:  LinReg ,  StatLearn  Logistic Regression:  StatLearn  Poisson Regression:  StatLearn  Support Vector Machines:  StatLearn  Quantile Regression:  StatLearn ,  QuantReg  Huber Loss Regression:  StatLearn  L1 Loss Regression:  StatLearn", 
            "title": "Predictive Modeling"
        }, 
        {
            "location": "/#experimental-features", 
            "text": "K-Means clustering:  KMeans  Bootstrapping:  BernoulliBootstrap ,  PoissonBootstrap  Approximate count of distinct elements:  HyperLogLog  Visualizing value of OnlineStats:  TracePlot ,  CompareTracePlot", 
            "title": "Experimental Features"
        }, 
        {
            "location": "/api/", 
            "text": "API\n\n\nBernoulliBootstrap\n\n\nBernoulliBootstrap(o::OnlineStat, f::Function, r::Int = 1000)\n\n\nCreate a double-or-nothing bootstrap using \nr\n replicates of \no\n for estimate \nf(o)\n\n\nExample:\n\n\nBernoulliBootstrap(Mean(), mean, 1000)\n\n\n\n\nTop\n\n\nBiasMatrix\n\n\nAdda bias/intercept term to a matrix on the fly without creating or copying data:\n\n\n\n\nBiasMatrix(rand(10,5))\n is roughly equivalent to \nhcat(rand(10,5), ones(10))\n\n\n\n\nTop\n\n\nBiasVector\n\n\nAdd a bias/intercept term to a vector on the fly without creating or copying data:\n\n\n\n\nBiasVector(rand(10))\n is roughly equivalent to \nvcat(rand(10), 1.0)\n\n\n\n\nTop\n\n\nBoundedEqualWeight\n\n\nOne of the \nWeight\n types.  Uses \nEqualWeight\n until reaching \n\u03bb = 2 / (1 + lookback)\n, then weights are held constant.\n\n\n\n\nBoundedEqualWeight(\u03bb::Float64)\n\n\nBoundedEqualWeight(lookback::Int)\n\n\n\n\nTop\n\n\nCovMatrix\n\n\nCovariance matrix, similar to \ncov(x)\n.\n\n\no = CovMatrix(x, EqualWeight())\no = CovMatrix(x)\nfit!(o, x2)\n\ncor(o)\ncov(o)\nmean(o)\nvar(o)\n\n\n\n\nTop\n\n\nDiff\n\n\nTrack the last value and the last difference.\n\n\no = Diff()\no = Diff(y)\n\n\n\n\nTop\n\n\nDiffs\n\n\nTrack the last value and the last difference for multiple series.  Ignores \nWeight\n.\n\n\no = Diffs()\no = Diffs(y)\n\n\n\n\nTop\n\n\nEqualWeight\n\n\nOne of the \nWeight\n types.  Observations are weighted equally.  For analytical updates, the online algorithm will give results equal to the offline version.\n\n\n\n\nEqualWeight()\n\n\n\n\nTop\n\n\nExponentialWeight\n\n\nOne of the \nWeight\n types.  Updates are performed with a constant weight \n\u03bb = 2 / (1 + lookback)\n.\n\n\n\n\nExponentialWeight(\u03bb::Float64)\n\n\nExponentialWeight(lookback::Int)\n\n\n\n\nTop\n\n\nExtrema\n\n\nExtrema (maximum and minimum).\n\n\no = Extrema(y)\nfit!(o, y2)\nextrema(o)\n\n\n\n\nTop\n\n\nFitCategorical\n\n\nFind the proportions for each unique input.  Categories are sorted by proportions. Ignores \nWeight\n.\n\n\no = FitCategorical(y)\n\n\n\n\nTop\n\n\nHyperLogLog\n\n\nHyperLogLog(b)\n\n\nApproximate count of distinct elements.  \nHyperLogLog\n differs from other OnlineStats in that any input to \nfit!(o::HyperLogLog, input)\n is considered a singleton.  Thus, a vector of inputs must be done by:\n\n\no = HyperLogLog(4)\nfor yi in y\n    fit!(o, yi)\nend\n\n\n\n\nTop\n\n\nKMeans\n\n\nApproximate K-Means clustering of multivariate data.\n\n\no = KMeans(y, 3, LearningRate())\nvalue(o)\n\n\n\n\nTop\n\n\nLearningRate\n\n\nOne of the \nWeight\n types.  It's primary use is for the OnlineStats that use stochastic approximation (\nStatLearn\n, \nQuantReg\n, \nQuantileMM\n, \nQuantileSGD\n, \nNormalMix\n, and \nKMeans\n).  The weight at update \nt\n is \n1 / t ^ r\n.  When weights reach \n\u03bb\n, they are held consant.  Compare to \nLearningRate2\n.\n\n\n\n\nLearningRate(r = 0.5, \u03bb = 0.0)\n\n\n\n\nTop\n\n\nLearningRate2\n\n\nOne of the \nWeight\n types.  It's primary use is for the OnlineStats that use stochastic approximation (\nStatLearn\n, \nQuantReg\n, \nQuantileMM\n, \nQuantileSGD\n, \nNormalMix\n, and \nKMeans\n).  The weight at update \nt\n is \n1 / (1 + c * (t - 1))\n.  When weights reach \n\u03bb\n, they are held consant.  Compare to \nLearningRate\n.\n\n\n\n\nLearningRate2(c = 0.5, \u03bb = 0.0)\n\n\n\n\nTop\n\n\nLinReg\n\n\nAnalytical Linear Regression.\n\n\nWith \nEqualWeight\n, this is equivalent to offline linear regression.\n\n\nusing OnlineStats, StatsBase\no = LinReg(x, y, wgt = EqualWeight())\ncoef(o)\ncoeftable(o)\nvcov(o)\nstderr(o)\npredict(o, x)\nconfint(o, .95)\n\n\n\n\nTop\n\n\nMean\n\n\nMean of a single series.\n\n\ny = randn(100)\no = Mean()\nfit!(o, y)\nmean(o)  # return the mean\ncenter(o, 0.5)  # returns 0.5 - mean(o)\n\n\n\n\nTop\n\n\nMeans\n\n\nMeans of multiple series, similar to \nmean(x, 1)\n.\n\n\nx = randn(1000, 5)\no = Means(5)\nfit!(o, x)\nmean(o)\n\n\n\n\nTop\n\n\nMoments\n\n\nUnivariate, first four moments.  Provides \nmean\n, \nvar\n, \nskewness\n, \nkurtosis\n\n\no = Moments(x, EqualWeight())\no = Moments(x)\nfit!(o, x2)\n\nmean(o)\nvar(o)\nstd(o)\nStatsBase.skewness(o)\nStatsBase.kurtosis(o)\n\n\n\n\nTop\n\n\nNormalMix\n\n\nNormal Mixture of \nk\n components via an online EM algorithm.  \nstart\n is a keyword argument specifying the initial parameters.\n\n\no = NormalMix(2, LearningRate(); start = MixtureModel(Normal, [(0, 1), (3, 1)]))\nmean(o)\nvar(o)\nstd(o)\n\n\n\n\nTop\n\n\nQuantReg\n\n\nOnline MM Algorithm for Quantile Regression.\n\n\nTop\n\n\nQuantileMM\n\n\nApproximate quantiles via an online MM algorithm.  Typically more accurate than \nQuantileSGD\n.\n\n\no = QuantileMM(y, LearningRate())\no = QuantileMM(y, tau = [.25, .5, .75])\nfit!(o, y2)\n\n\n\n\nTop\n\n\nQuantileSGD\n\n\nApproximate quantiles via stochastic gradient descent.\n\n\no = QuantileSGD(y, LearningRate())\no = QuantileSGD(y, tau = [.25, .5, .75])\nfit!(o, y2)\n\n\n\n\nTop\n\n\nStatLearn\n\n\nOnline statistical learning algorithms.\n\n\n\n\nStatLearn(p)\n\n\nStatLearn(x, y)\n\n\nStatLearn(x, y, b)\n\n\n\n\nThe model is defined by:\n\n\nModelDefinition\n\n\n\n\nL2Regression()\n     - Squared error loss.  Default.\n\n\nL1Regression()\n     - Absolute loss\n\n\nLogisticRegression()\n     - Model for data in {0, 1}\n\n\nPoissonRegression()\n     - Model count data {0, 1, 2, 3, ...}\n\n\nQuantileRegression(\u03c4)\n     - Model conditional quantiles\n\n\nSVMLike()\n     - For data in {-1, 1}.  Perceptron with \nNoPenalty\n. SVM with \nRidgePenalty\n.\n\n\nHuberRegression(\u03b4)\n     - Robust Huber loss\n\n\n\n\nPenalty\n\n\n\n\nNoPenalty()\n     - No penalty.  Default.\n\n\nRidgePenalty(\u03bb)\n     - Ridge regularization: \ndot(\u03b2, \u03b2)\n\n\nLassoPenalty(\u03bb)\n     - Lasso regularization: \nsumabs(\u03b2)\n\n\nElasticNetPenalty(\u03bb, \u03b1)\n     - Ridge/LASSO weighted average.  \n\u03b1 = 0\n is Ridge, \n\u03b1 = 1\n is LASSO.\n\n\nSCADPenalty(\u03bb, a = 3.7)\n     - Smoothly clipped absolute deviation penalty.  Essentially LASSO with less bias     for larger coefficients.\n\n\n\n\nAlgorithm\n\n\n\n\nSGD()\n     - Stochastic gradient descent.  Default.\n\n\nAdaGrad()\n     - Adaptive gradient method. Ignores \nWeight\n.\n\n\nAdaDelta()\n     - Extension of AdaGrad.  Ignores \nWeight\n.\n\n\nRDA()\n     - Regularized dual averaging with ADAGRAD.  Ignores \nWeight\n.\n\n\nMMGrad()\n     - Experimental online MM gradient method.\n\n\n\n\nNote:\n The order of the \nModelDefinition\n, \nPenalty\n, and \nAlgorithm\n arguments don't matter.\n\n\nStatLearn(x, y)\nStatLearn(x, y, AdaGrad())\nStatLearn(x, y, MMGrad(), LearningRate(.5))\nStatLearn(x, y, 10, LearningRate(.7), RDA(), SVMLike(), RidgePenalty(.1))\n\n\n\n\nTop\n\n\nSum\n\n\nTrack the running sum.  Ignores \nWeight\n.\n\n\no = Sum()\no = Sum(y)\n\n\n\n\nTop\n\n\nSums\n\n\nTrack the running sum for multiple series.  Ignores \nWeight\n.\n\n\no = Sums()\no = Sums(y)\n\n\n\n\nTop\n\n\nTwoWayInteractionMatrix\n\n\nAdd second-order interaction terms on the fly without creating or copying data:\n\n\n\n\nTwoWayInteractionMatrix(rand(n, p))\n \"adds\" the \nbinomial(p, 2)\n interaction terms to each row\n\n\n\n\nTop\n\n\nTwoWayInteractionVector\n\n\nAdd second-order interaction terms on the fly without creating or copying data:\n\n\n\n\nTwoWayInteractionVector(rand(p))\n \"adds\" the \nbinomial(p, 2)\n interaction terms\n\n\n\n\nTop\n\n\nVariance\n\n\nUnivariate variance.\n\n\ny = randn(100)\no = Variance(y)\nmean(o)\nvar(o)\nstd(o)\n\n\n\n\nTop\n\n\nVariances\n\n\nVariances of a multiple series, similar to \nvar(x, 1)\n.\n\n\no = Variances(x, EqualWeight())\no = Variances(x)\nfit!(o, x2)\n\nmean(o)\nvar(o)\nstd(o)\n\n\n\n\nTop\n\n\nfit!\n\n\nUpdate an OnlineStat with more data.  Additional arguments after the input data provide extra control over how the updates are done.\n\n\ny = randn(100)\no = Mean()\n\nfit!(o, y)      # standard usage\n\nfit!(o, y, 10)  # update in minibatches of size 10\n\nfit!(o, y, .1)  # update using weight .1 for each observation\n\nwts = rand(100)\nfit!(o, y, wts) # update observation i using wts[i]\n\n\n\n\nTop\n\n\nfitdistribution\n\n\nEstimate the parameters of a distribution.\n\n\nusing Distributions\n# Univariate distributions\no = fitdistribution(Beta, y)\no = fitdistribution(Categorical, y)  # ignores Weight\no = fitdistribution(Cauchy, y)\no = fitdistribution(Gamma, y)\no = fitdistribution(LogNormal, y)\no = fitdistribution(Normal, y)\nmean(o)\nvar(o)\nstd(o)\nparams(o)\n\n# Multivariate distributions\no = fitdistribution(Multinomial, x)\no = fitdistribution(MvNormal, x)\nmean(o)\nvar(o)\nstd(o)\ncov(o)\n\n\n\n\nTop\n\n\nmap_rows\n\n\nPerform operations on data in blocks.\n\n\nmap_rows(f::Function, b::Integer, data...)\n\n\nThis function iteratively feeds the \nf\n blocks of \nb\n observations from \ndata\n. The most common usage is with \ndo\n blocks:\n\n\n# Example 1\ny = randn(50)\no = Variance()\nmap_rows(10, y) do yi\n    fit!(o, yi)\n    println(\nUpdated with another batch!\n)\nend\ndisplay(o)\n\n# Example 2\nx = randn(100, 5)\ny = randn(100)\no = LinReg(x, y)\nmap_rows(10, x, y) do xi, yi\n    fit!(o, xi, yi)\n    println(\nUpdated with another batch!\n)\nend\ndisplay(o)\n\n\n\n\nTop\n\n\nnobs\n\n\nnobs(obj::StatisticalModel)\n\n\nReturns the number of independent observations on which the model was fitted. Be careful when using this information, as the definition of an independent observation may vary depending on the model, on the format used to pass the data, on the sampling plan (if specified), etc.\n\n\nTop\n\n\nsweep!\n\n\nsweep!(A, k, inv = false)\n, \nsweep!(A, k, v, inv = false)\n\n\nSymmetric sweep operator of the matrix \nA\n on element \nk\n.  \nA\n is overwritten. \ninv = true\n will perform the inverse sweep.  Only the upper triangle is read and swept.\n\n\nAn optional vector \nv\n can be provided to avoid memory allocation. This requires \nlength(v) == size(A, 1)\n.  Both \nA\n and \nv\n will be overwritten.\n\n\nx = randn(100, 10)\nxtx = x'x\nsweep!(xtx, 1)\nsweep!(xtx, 1, true)\n\n\n\n\nTop\n\n\nvalue\n\n\nThe associated value of an OnlineStat.\n\n\no = Mean()\nvalue(o)\n\n\n\n\nTop", 
            "title": "API"
        }, 
        {
            "location": "/api/#api", 
            "text": "", 
            "title": "API"
        }, 
        {
            "location": "/api/#bernoullibootstrap", 
            "text": "BernoulliBootstrap(o::OnlineStat, f::Function, r::Int = 1000)  Create a double-or-nothing bootstrap using  r  replicates of  o  for estimate  f(o)  Example:  BernoulliBootstrap(Mean(), mean, 1000)  Top", 
            "title": "BernoulliBootstrap"
        }, 
        {
            "location": "/api/#biasmatrix", 
            "text": "Adda bias/intercept term to a matrix on the fly without creating or copying data:   BiasMatrix(rand(10,5))  is roughly equivalent to  hcat(rand(10,5), ones(10))   Top", 
            "title": "BiasMatrix"
        }, 
        {
            "location": "/api/#biasvector", 
            "text": "Add a bias/intercept term to a vector on the fly without creating or copying data:   BiasVector(rand(10))  is roughly equivalent to  vcat(rand(10), 1.0)   Top", 
            "title": "BiasVector"
        }, 
        {
            "location": "/api/#boundedequalweight", 
            "text": "One of the  Weight  types.  Uses  EqualWeight  until reaching  \u03bb = 2 / (1 + lookback) , then weights are held constant.   BoundedEqualWeight(\u03bb::Float64)  BoundedEqualWeight(lookback::Int)   Top", 
            "title": "BoundedEqualWeight"
        }, 
        {
            "location": "/api/#covmatrix", 
            "text": "Covariance matrix, similar to  cov(x) .  o = CovMatrix(x, EqualWeight())\no = CovMatrix(x)\nfit!(o, x2)\n\ncor(o)\ncov(o)\nmean(o)\nvar(o)  Top", 
            "title": "CovMatrix"
        }, 
        {
            "location": "/api/#diff", 
            "text": "Track the last value and the last difference.  o = Diff()\no = Diff(y)  Top", 
            "title": "Diff"
        }, 
        {
            "location": "/api/#diffs", 
            "text": "Track the last value and the last difference for multiple series.  Ignores  Weight .  o = Diffs()\no = Diffs(y)  Top", 
            "title": "Diffs"
        }, 
        {
            "location": "/api/#equalweight", 
            "text": "One of the  Weight  types.  Observations are weighted equally.  For analytical updates, the online algorithm will give results equal to the offline version.   EqualWeight()   Top", 
            "title": "EqualWeight"
        }, 
        {
            "location": "/api/#exponentialweight", 
            "text": "One of the  Weight  types.  Updates are performed with a constant weight  \u03bb = 2 / (1 + lookback) .   ExponentialWeight(\u03bb::Float64)  ExponentialWeight(lookback::Int)   Top", 
            "title": "ExponentialWeight"
        }, 
        {
            "location": "/api/#extrema", 
            "text": "Extrema (maximum and minimum).  o = Extrema(y)\nfit!(o, y2)\nextrema(o)  Top", 
            "title": "Extrema"
        }, 
        {
            "location": "/api/#fitcategorical", 
            "text": "Find the proportions for each unique input.  Categories are sorted by proportions. Ignores  Weight .  o = FitCategorical(y)  Top", 
            "title": "FitCategorical"
        }, 
        {
            "location": "/api/#hyperloglog", 
            "text": "HyperLogLog(b)  Approximate count of distinct elements.   HyperLogLog  differs from other OnlineStats in that any input to  fit!(o::HyperLogLog, input)  is considered a singleton.  Thus, a vector of inputs must be done by:  o = HyperLogLog(4)\nfor yi in y\n    fit!(o, yi)\nend  Top", 
            "title": "HyperLogLog"
        }, 
        {
            "location": "/api/#kmeans", 
            "text": "Approximate K-Means clustering of multivariate data.  o = KMeans(y, 3, LearningRate())\nvalue(o)  Top", 
            "title": "KMeans"
        }, 
        {
            "location": "/api/#learningrate", 
            "text": "One of the  Weight  types.  It's primary use is for the OnlineStats that use stochastic approximation ( StatLearn ,  QuantReg ,  QuantileMM ,  QuantileSGD ,  NormalMix , and  KMeans ).  The weight at update  t  is  1 / t ^ r .  When weights reach  \u03bb , they are held consant.  Compare to  LearningRate2 .   LearningRate(r = 0.5, \u03bb = 0.0)   Top", 
            "title": "LearningRate"
        }, 
        {
            "location": "/api/#learningrate2", 
            "text": "One of the  Weight  types.  It's primary use is for the OnlineStats that use stochastic approximation ( StatLearn ,  QuantReg ,  QuantileMM ,  QuantileSGD ,  NormalMix , and  KMeans ).  The weight at update  t  is  1 / (1 + c * (t - 1)) .  When weights reach  \u03bb , they are held consant.  Compare to  LearningRate .   LearningRate2(c = 0.5, \u03bb = 0.0)   Top", 
            "title": "LearningRate2"
        }, 
        {
            "location": "/api/#linreg", 
            "text": "Analytical Linear Regression.  With  EqualWeight , this is equivalent to offline linear regression.  using OnlineStats, StatsBase\no = LinReg(x, y, wgt = EqualWeight())\ncoef(o)\ncoeftable(o)\nvcov(o)\nstderr(o)\npredict(o, x)\nconfint(o, .95)  Top", 
            "title": "LinReg"
        }, 
        {
            "location": "/api/#mean", 
            "text": "Mean of a single series.  y = randn(100)\no = Mean()\nfit!(o, y)\nmean(o)  # return the mean\ncenter(o, 0.5)  # returns 0.5 - mean(o)  Top", 
            "title": "Mean"
        }, 
        {
            "location": "/api/#means", 
            "text": "Means of multiple series, similar to  mean(x, 1) .  x = randn(1000, 5)\no = Means(5)\nfit!(o, x)\nmean(o)  Top", 
            "title": "Means"
        }, 
        {
            "location": "/api/#moments", 
            "text": "Univariate, first four moments.  Provides  mean ,  var ,  skewness ,  kurtosis  o = Moments(x, EqualWeight())\no = Moments(x)\nfit!(o, x2)\n\nmean(o)\nvar(o)\nstd(o)\nStatsBase.skewness(o)\nStatsBase.kurtosis(o)  Top", 
            "title": "Moments"
        }, 
        {
            "location": "/api/#normalmix", 
            "text": "Normal Mixture of  k  components via an online EM algorithm.   start  is a keyword argument specifying the initial parameters.  o = NormalMix(2, LearningRate(); start = MixtureModel(Normal, [(0, 1), (3, 1)]))\nmean(o)\nvar(o)\nstd(o)  Top", 
            "title": "NormalMix"
        }, 
        {
            "location": "/api/#quantreg", 
            "text": "Online MM Algorithm for Quantile Regression.  Top", 
            "title": "QuantReg"
        }, 
        {
            "location": "/api/#quantilemm", 
            "text": "Approximate quantiles via an online MM algorithm.  Typically more accurate than  QuantileSGD .  o = QuantileMM(y, LearningRate())\no = QuantileMM(y, tau = [.25, .5, .75])\nfit!(o, y2)  Top", 
            "title": "QuantileMM"
        }, 
        {
            "location": "/api/#quantilesgd", 
            "text": "Approximate quantiles via stochastic gradient descent.  o = QuantileSGD(y, LearningRate())\no = QuantileSGD(y, tau = [.25, .5, .75])\nfit!(o, y2)  Top", 
            "title": "QuantileSGD"
        }, 
        {
            "location": "/api/#statlearn", 
            "text": "Online statistical learning algorithms.   StatLearn(p)  StatLearn(x, y)  StatLearn(x, y, b)   The model is defined by:", 
            "title": "StatLearn"
        }, 
        {
            "location": "/api/#modeldefinition", 
            "text": "L2Regression()      - Squared error loss.  Default.  L1Regression()      - Absolute loss  LogisticRegression()      - Model for data in {0, 1}  PoissonRegression()      - Model count data {0, 1, 2, 3, ...}  QuantileRegression(\u03c4)      - Model conditional quantiles  SVMLike()      - For data in {-1, 1}.  Perceptron with  NoPenalty . SVM with  RidgePenalty .  HuberRegression(\u03b4)      - Robust Huber loss", 
            "title": "ModelDefinition"
        }, 
        {
            "location": "/api/#penalty", 
            "text": "NoPenalty()      - No penalty.  Default.  RidgePenalty(\u03bb)      - Ridge regularization:  dot(\u03b2, \u03b2)  LassoPenalty(\u03bb)      - Lasso regularization:  sumabs(\u03b2)  ElasticNetPenalty(\u03bb, \u03b1)      - Ridge/LASSO weighted average.   \u03b1 = 0  is Ridge,  \u03b1 = 1  is LASSO.  SCADPenalty(\u03bb, a = 3.7)      - Smoothly clipped absolute deviation penalty.  Essentially LASSO with less bias     for larger coefficients.", 
            "title": "Penalty"
        }, 
        {
            "location": "/api/#algorithm", 
            "text": "SGD()      - Stochastic gradient descent.  Default.  AdaGrad()      - Adaptive gradient method. Ignores  Weight .  AdaDelta()      - Extension of AdaGrad.  Ignores  Weight .  RDA()      - Regularized dual averaging with ADAGRAD.  Ignores  Weight .  MMGrad()      - Experimental online MM gradient method.   Note:  The order of the  ModelDefinition ,  Penalty , and  Algorithm  arguments don't matter.  StatLearn(x, y)\nStatLearn(x, y, AdaGrad())\nStatLearn(x, y, MMGrad(), LearningRate(.5))\nStatLearn(x, y, 10, LearningRate(.7), RDA(), SVMLike(), RidgePenalty(.1))  Top", 
            "title": "Algorithm"
        }, 
        {
            "location": "/api/#sum", 
            "text": "Track the running sum.  Ignores  Weight .  o = Sum()\no = Sum(y)  Top", 
            "title": "Sum"
        }, 
        {
            "location": "/api/#sums", 
            "text": "Track the running sum for multiple series.  Ignores  Weight .  o = Sums()\no = Sums(y)  Top", 
            "title": "Sums"
        }, 
        {
            "location": "/api/#twowayinteractionmatrix", 
            "text": "Add second-order interaction terms on the fly without creating or copying data:   TwoWayInteractionMatrix(rand(n, p))  \"adds\" the  binomial(p, 2)  interaction terms to each row   Top", 
            "title": "TwoWayInteractionMatrix"
        }, 
        {
            "location": "/api/#twowayinteractionvector", 
            "text": "Add second-order interaction terms on the fly without creating or copying data:   TwoWayInteractionVector(rand(p))  \"adds\" the  binomial(p, 2)  interaction terms   Top", 
            "title": "TwoWayInteractionVector"
        }, 
        {
            "location": "/api/#variance", 
            "text": "Univariate variance.  y = randn(100)\no = Variance(y)\nmean(o)\nvar(o)\nstd(o)  Top", 
            "title": "Variance"
        }, 
        {
            "location": "/api/#variances", 
            "text": "Variances of a multiple series, similar to  var(x, 1) .  o = Variances(x, EqualWeight())\no = Variances(x)\nfit!(o, x2)\n\nmean(o)\nvar(o)\nstd(o)  Top", 
            "title": "Variances"
        }, 
        {
            "location": "/api/#fit", 
            "text": "Update an OnlineStat with more data.  Additional arguments after the input data provide extra control over how the updates are done.  y = randn(100)\no = Mean()\n\nfit!(o, y)      # standard usage\n\nfit!(o, y, 10)  # update in minibatches of size 10\n\nfit!(o, y, .1)  # update using weight .1 for each observation\n\nwts = rand(100)\nfit!(o, y, wts) # update observation i using wts[i]  Top", 
            "title": "fit!"
        }, 
        {
            "location": "/api/#fitdistribution", 
            "text": "Estimate the parameters of a distribution.  using Distributions\n# Univariate distributions\no = fitdistribution(Beta, y)\no = fitdistribution(Categorical, y)  # ignores Weight\no = fitdistribution(Cauchy, y)\no = fitdistribution(Gamma, y)\no = fitdistribution(LogNormal, y)\no = fitdistribution(Normal, y)\nmean(o)\nvar(o)\nstd(o)\nparams(o)\n\n# Multivariate distributions\no = fitdistribution(Multinomial, x)\no = fitdistribution(MvNormal, x)\nmean(o)\nvar(o)\nstd(o)\ncov(o)  Top", 
            "title": "fitdistribution"
        }, 
        {
            "location": "/api/#map_rows", 
            "text": "Perform operations on data in blocks.  map_rows(f::Function, b::Integer, data...)  This function iteratively feeds the  f  blocks of  b  observations from  data . The most common usage is with  do  blocks:  # Example 1\ny = randn(50)\no = Variance()\nmap_rows(10, y) do yi\n    fit!(o, yi)\n    println( Updated with another batch! )\nend\ndisplay(o)\n\n# Example 2\nx = randn(100, 5)\ny = randn(100)\no = LinReg(x, y)\nmap_rows(10, x, y) do xi, yi\n    fit!(o, xi, yi)\n    println( Updated with another batch! )\nend\ndisplay(o)  Top", 
            "title": "map_rows"
        }, 
        {
            "location": "/api/#nobs", 
            "text": "nobs(obj::StatisticalModel)  Returns the number of independent observations on which the model was fitted. Be careful when using this information, as the definition of an independent observation may vary depending on the model, on the format used to pass the data, on the sampling plan (if specified), etc.  Top", 
            "title": "nobs"
        }, 
        {
            "location": "/api/#sweep", 
            "text": "sweep!(A, k, inv = false) ,  sweep!(A, k, v, inv = false)  Symmetric sweep operator of the matrix  A  on element  k .   A  is overwritten.  inv = true  will perform the inverse sweep.  Only the upper triangle is read and swept.  An optional vector  v  can be provided to avoid memory allocation. This requires  length(v) == size(A, 1) .  Both  A  and  v  will be overwritten.  x = randn(100, 10)\nxtx = x'x\nsweep!(xtx, 1)\nsweep!(xtx, 1, true)  Top", 
            "title": "sweep!"
        }, 
        {
            "location": "/api/#value", 
            "text": "The associated value of an OnlineStat.  o = Mean()\nvalue(o)  Top", 
            "title": "value"
        }
    ]
}